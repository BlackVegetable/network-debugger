\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{epsfig,xspace,url}
\usepackage{authblk}


\title{\huge \bf DSM: an SDN-Enhanced Debugging Tool for Distributed Systems  \\}
\author{ \Large Devin Ekins, Hadeel Maryoosh, Yue Fei, Eric Eide}
\affil{\large School of Computing,
 University of Utah}
\date{}

\begin{document}

\maketitle

\section*{\LARGE Abstract}

( here we write the abstract)

\section*{Categories and Subject Descriptors}

( If applicable)

\section*{Keyword}

( any keyword in the paper)

\section{\LARGE Introduction}


Debugging a distributed system can be a very difficult and challenging task.  A distributed
system consists of multiple processes, running across multiple computers, that
cooperate across a network to perform some task. This debugging challenge is due to scalability of these systems which is one of their standard characteristics. An example is a web site
implemented as a distributed system including a load balancer, a set of HTTP
servers, and a set of database servers.  If the system as a whole misbehaves,
it can be tricky to figure out which of the many pieces of the system is at
fault.  Even monitoring the communication between the parts of the system can
be a challenge.

To help the users simplifying the debugging process of these systems, many proposals and tools have been presented (as discussed in section 7.) However, these tools tend to be limited by a narrow domain (such as a network or remote sensors), require massive amounts of data and statistical analysis to operate, or have other limitations that keep them from being general-purpose network debuggers. One idea used in much of the related work is that of a user-defined script to model the intended execution of a system; we leverage that notion in our solution as well.

We present our approach, the "Debugging State Machine" (DSM), an SDN-Enhanced Debugging Tool for Distributed Systems, which is a new way to observe and control events between the components of a distributed system using Software-Defined networking (SDN) and the "Debugging State Machine Language" (DSML), a domain specific scripting language. This project was motivated by the existing need for a simple, practical debugging tool that met the following characteristics: 1) It exposes only relevant traffic to the debugger, simplifying the debugging process and performing far better in the face of large amounts of network traffic.  2) Users can make use of debugging concepts analagous to those found in a traditional debugger such as a breakpoint and a stacktrace. 3) Users can specify the expected behavior of the system and capture information regarding violations to those expectations.

Using these characteristics as our goals, we built and designed our DSM approach leveraging the OpenFlow interface to meet the performance criteria, a controller capable of producing contextual information about its status similar to a stacktrace, and our DSML to allow the user to custom-tailor the debugger to their protocol's expectations.

\section{\LARGE Solution}

Our system allows users to define network protocols in terms of states of a Definite Finite Automaton (DFA) using our Debugging State Machine Language (DSML). Because explaining network protocols often involves a graphical representation as a DFA, we felt this would be a good way to model network status from the perspective of a debugger as well.

DSML supports the definition of states as well as the transitions between them. Transitions are defined by one or more matching operations which performs deep packet inspection to match a field's contents (or a substring of its contents) against a value. Alternatively, a state transition may be defined by a timeout wherein no matching packets were encountered within a given amount of time.

As deep packet expansion is expensive, we provide a mechanism to avoid inspecting the entirety of network traffic. To avoid performing these expensive operations on every packet sent through the network, DSML also supports the use of OpenFlow filters to funnel relevant traffic to the debugger itself and allow other network traffic to continue onto its original destination unimpeded. This funneling is performed at line rate. For example, a user may know that this debugger only need to consider packets with destination port of 80 for HTTP traffic. Then the debugger only performs inspection on this subset of packets.

Importantly, DSML allows for the use of side-effect operations which include logging to a file or printing to a console relevant information as well as displaying its equivalent of a stack trace. This stack trace is a history of network states that were traversed to arrive at the current position as well as the packets that were matched on for each transition. This allows users to get the feedback they need to make informed debugging decisions.

The DSML protocol script written by the user is read into a compiler to produce the Debugger State Machine (DSM) which then may be run on a controller with an OpenFlow-enabled switch to begin debugging network traffic. Switches need not support OpenFlow to send traffic to the DSM, but the optimization gained by the OpenFlow filters will not be present if OpenFlow is not supported on the switch.

\section{\LARGE Implementation}

\begin{itemize}
  \item DSML parsing
  \item DSM DPI
  \item DSML Engine (concatenation of code & compiled output)
  \item POX Controller
  \item Controller Sniffing via Scapy
  \item Connection between switch and controller
  \item Stacktrace display?
\end{itemize}

\section{\LARGE Evaluation}

\begin{itemize}
  \item Compiler performance
  \item DSML Ease of Use
  \item Run-time performance
  \item DSM Usefulness
\end{itemize}

\section{\LARGE Future Work}

\begin{itemize}
  \item Syntactic Sugar
  \item Composition of Protocols
  \item Parallel DSMs
  \item Support for non-OF enabled switches
\end{itemize}

\section{\LARGE Related Work}

The topic of debugging distributed systems has long been studied\cite{miller1988breakpoints}, but as the scale and functionalities expected within a networked system continue to grow, the components of today's distributed system are not only deployed in a distributed fashion but are also difficult to reason about as every component within a distributed system seems to be written in a different language.\cite{chow2014mystery}. While there were good examples to debug specific distributed applications within particular domains, such as within MPI applications\cite{dryden2014pgdb}, the need for a general purpose debugging tool had not been previously met.

Using Big Data sampling techniques\cite{chow2014mystery} to understand and model the behavior of a distributed system is a good idea in large-scale settings as Big Data learning functions well when sufficient input is available. However, for smaller-scale settings (such as those that might be addressed by our DSM), it may not have enough input for the learning algorithms to function.

There are multiple ways to observe and monitor distributed systems and the applications running on them. lprof\cite{zhao2014lprof} uses static analysis of binary source code to infer how logs can be parsed and correlated with individual requests for performance measurements. Minerva\cite{sommer2013minerva} a remote sensor network debugging tool introduced a synchronous "breakpoint" while avoiding modifying the execution timing of its nodes. This is a hardware solution involving attaching a debug board to each node.

In the paper "On the design of a pervasive debugger" by Ho et al.\cite{ho2005design} distributed applications are executed in a virtual environment where the debugger can simulate the execution of the actual program but allow for breakpoints and run-time examination not possible outside of a virtualized environment. Declarative Tracepoints\cite{cao2008declarative} provides a debugging system specifically for sensor networks that allows users to insert action-associated checkpoints in a SQL-like script in a manner similar to our DSML scripts.

Some instructive research titled "Data centric highly parallel debugging" by Abramson et al.\cite{abramson2010data} focused on having users describe their expectations in terms of the state of data independent of individual process execution. Work by Marceau et al. on "A Dataflow Language for Scriptable Debugging"\cite{marceau2004dataflow} demonstrates the power of user-defined scripts for expressing debugging goals.


\section{\LARGE Conclusion}

\begin{itemize}
  \item Clearly, the internet is made for pizza.
  \item Merits of various pizza formats
  \item IEEE standard for secure pizza distribution protocol
\end{itemize}

{
  \footnotesize
  \small
  \bibliographystyle{acm}
  \bibliography{biblio}
}
\end{document}
